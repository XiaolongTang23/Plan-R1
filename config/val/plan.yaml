univeral_config: &univeral_config
  mode: "plan"
  token_dict_path: tokens/tokens_1024.pt
  num_tokens: 1024
  interval: 5
  num_historical_steps: 20
  num_future_steps: 80

dataset:
  <<: *univeral_config
  root: ../nuplan/dataset
  dir: train

dataloader:
  batch_size: 4
  num_workers: 8
  pin_memory: True
  persistent_workers: True

trainer:
  ckpt_path: ckpts/fine-tuning.ckpt
  accelerator: "gpu"
  devices: 8

model:
  <<: *univeral_config
  hidden_dim: 128
  num_heads: 8
  num_attn_layers: 6
  num_hops: 4
  agent_radius: 60
  polygon_radius: 30
  pred_top_k: 1
  plan_top_k: 1
  rollout_top_k: 50
  val_visualization: False